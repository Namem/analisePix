{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "061f08bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- FASE 1: Carregamento e Exploração dos Dados ---\n",
      "============================================================\n",
      "\n",
      "[OBJETIVO] O primeiro passo é carregar nosso arquivo CSV para a memória e fazer uma rápida inspeção para entender sua estrutura.\n",
      "\n",
      "[SUCESSO] Arquivo CSV carregado com sucesso.\n",
      "O que encontramos? Um total de 10000 linhas (transações) e 15 colunas (atributos).\n",
      "\n",
      "[AMOSTRA] Abaixo estão as 5 primeiras linhas para termos uma ideia dos dados:\n",
      "                             EndToEndId             DataHora    Valor Moeda  \\\n",
      "0  f094cb2c-2a73-463c-b60e-0c57262051e4  2025-06-26 21:07:56  4658.86   BRL   \n",
      "1  f0409769-741f-49bd-811a-3842cc8f54db  2025-06-23 17:47:56  3184.72   BRL   \n",
      "2  c1d22287-d343-4c91-b03d-dbe9a69695b3  2025-06-27 15:58:56  1054.48   BRL   \n",
      "3  254423a9-50b7-41b1-a099-e8cf6232bab4  2025-07-15 13:59:56  3564.76   BRL   \n",
      "4  f1c5e3e7-bc62-4f18-91e3-c8f92aebdd59  2025-06-23 06:19:56    15.16   BRL   \n",
      "\n",
      "           Pagador_Nome    Pagador_CPF_CNPJ     Pagador_Banco  \\\n",
      "0        Nathan Cardoso  13.253.967/0001-95       BTG Pactual   \n",
      "1       Benjamin Barros      781.515.544-64   Banco do Brasil   \n",
      "2          Lorena Cunha      444.922.709-96       Banco Inter   \n",
      "3      Sra. Luana Pinto      468.153.904-62       Banco Safra   \n",
      "4  João Miguel da Cunha  83.407.726/0001-68  Santander Brasil   \n",
      "\n",
      "         Recebedor_Nome  Recebedor_CPF_CNPJ          Recebedor_Banco  \\\n",
      "0     Ana Beatriz Ramos      901.361.580-95                   Nubank   \n",
      "1  Carlos Eduardo Nunes      288.758.607-72            Itaú Unibanco   \n",
      "2        Lívia Silveira      530.827.935-48                 Bradesco   \n",
      "3          Helena Pires      827.707.674-51  Caixa Econômica Federal   \n",
      "4  Dr. Luiz Felipe Dias  65.245.322/0001-12                  C6 Bank   \n",
      "\n",
      "                     ChavePix_Utilizada        TipoChave  \\\n",
      "0                       +559694830-9884         Telefone   \n",
      "1                       +552298530-1917         Telefone   \n",
      "2  c4f4c3c6-4d78-4946-b33e-4a1135711a8a  Chave Aleatória   \n",
      "3                     user421@email.com           E-mail   \n",
      "4                      user97@email.com           E-mail   \n",
      "\n",
      "                            Descricao     Status  Anomalia  \n",
      "0  Pagamento referente ao serviço 927  Concluída         0  \n",
      "1  Pagamento referente ao serviço 950   Pendente         0  \n",
      "2  Pagamento referente ao serviço 173   Pendente         0  \n",
      "3  Pagamento referente ao serviço 560  Estornada         0  \n",
      "4  Pagamento referente ao serviço 140   Pendente         0  \n",
      "\n",
      "[INFO] Agora, vamos verificar a estrutura técnica (tipos de dados, valores nulos) com o método .info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   EndToEndId          10000 non-null  object \n",
      " 1   DataHora            10000 non-null  object \n",
      " 2   Valor               10000 non-null  float64\n",
      " 3   Moeda               10000 non-null  object \n",
      " 4   Pagador_Nome        10000 non-null  object \n",
      " 5   Pagador_CPF_CNPJ    10000 non-null  object \n",
      " 6   Pagador_Banco       10000 non-null  object \n",
      " 7   Recebedor_Nome      10000 non-null  object \n",
      " 8   Recebedor_CPF_CNPJ  10000 non-null  object \n",
      " 9   Recebedor_Banco     10000 non-null  object \n",
      " 10  ChavePix_Utilizada  10000 non-null  object \n",
      " 11  TipoChave           10000 non-null  object \n",
      " 12  Descricao           10000 non-null  object \n",
      " 13  Status              10000 non-null  object \n",
      " 14  Anomalia            10000 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(13)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#           ANÁLISE DE PADRÕES DE ANOMALIA EM TRANSAÇÕES PIX\n",
    "# ==============================================================================\n",
    "# Este script foi projetado para ler um arquivo de transações, aprender os\n",
    "# perfis de transações anômalas e, em seguida, gerar um novo arquivo CSV\n",
    "# contendo apenas as transações suspeitas, com uma explicação do motivo.\n",
    "# ==============================================================================\n",
    "\n",
    "# Importação das bibliotecas necessárias\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# --- FASE 1: Carregamento e Exploração dos Dados ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- FASE 1: Carregamento e Exploração dos Dados ---\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[OBJETIVO] O primeiro passo é carregar nosso arquivo CSV para a memória e fazer uma rápida inspeção para entender sua estrutura.\")\n",
    "\n",
    "df = pd.read_csv('comprovantes_pix_10000_anomalias.csv', sep=';')\n",
    "print(f\"\\n[SUCESSO] Arquivo CSV carregado com sucesso.\")\n",
    "print(f\"O que encontramos? Um total de {df.shape[0]} linhas (transações) e {df.shape[1]} colunas (atributos).\")\n",
    "print(\"\\n[AMOSTRA] Abaixo estão as 5 primeiras linhas para termos uma ideia dos dados:\")\n",
    "print(df.head())\n",
    "print(\"\\n[INFO] Agora, vamos verificar a estrutura técnica (tipos de dados, valores nulos) com o método .info():\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "165a3dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- FASE 2: Preparação dos Dados e Criação de Novas Features ---\n",
      "============================================================\n",
      "\n",
      "[OBJETIVO] Dados brutos raramente estão prontos para análise. Vamos limpá-los e criar novas colunas (features) que ajudem o algoritmo a encontrar padrões.\n",
      "\n",
      "[AÇÃO] Convertendo a coluna 'DataHora' para um formato de data que o Python entende...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCESSO] 'DataHora' agora é uma coluna de data, permitindo extrair informações como hora, dia, etc.\n",
      "\n",
      "[AÇÃO] Criando uma nova coluna chamada 'periodo_dia' a partir da hora da transação...\n",
      "[SUCESSO] Coluna 'periodo_dia' criada. Isso transforma a hora (numérica) em uma categoria (texto), o que é ótimo para encontrar regras.\n",
      "[ANÁLISE] Vamos ver a distribuição de transações por período:\n",
      "periodo_dia\n",
      "Noite        2554\n",
      "Madrugada    2501\n",
      "Manha        2463\n",
      "Tarde        2458\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[AÇÃO] Criando a coluna 'valor_cat' para agrupar os valores das transações em faixas (Baixo, Medio, Alto)...\n",
      "[SUCESSO] Coluna 'valor_cat' criada. Agrupar valores ajuda o algoritmo a encontrar padrões como 'transações de baixo valor são suspeitas'.\n",
      "[ANÁLISE] Distribuição de transações por categoria de valor:\n",
      "valor_cat\n",
      "Alto     3326\n",
      "Baixo    3325\n",
      "Medio    3325\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- FASE 2: Preparação dos Dados e Criação de Novas Features ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- FASE 2: Preparação dos Dados e Criação de Novas Features ---\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[OBJETIVO] Dados brutos raramente estão prontos para análise. Vamos limpá-los e criar novas colunas (features) que ajudem o algoritmo a encontrar padrões.\")\n",
    "\n",
    "# Conversão da coluna de data e hora\n",
    "print(\"\\n[AÇÃO] Convertendo a coluna 'DataHora' para um formato de data que o Python entende...\")\n",
    "df['DataHora'] = pd.to_datetime(df['DataHora'], errors='coerce')\n",
    "df.dropna(subset=['DataHora'], inplace=True)\n",
    "print(\"[SUCESSO] 'DataHora' agora é uma coluna de data, permitindo extrair informações como hora, dia, etc.\")\n",
    "\n",
    "# Criação da coluna 'periodo_dia'\n",
    "print(\"\\n[AÇÃO] Criando uma nova coluna chamada 'periodo_dia' a partir da hora da transação...\")\n",
    "def get_periodo_dia(hour):\n",
    "    if 0 <= hour < 6: return 'Madrugada'\n",
    "    elif 6 <= hour < 12: return 'Manha'\n",
    "    elif 12 <= hour < 18: return 'Tarde'\n",
    "    else: return 'Noite'\n",
    "df['periodo_dia'] = df['DataHora'].dt.hour.apply(get_periodo_dia)\n",
    "print(\"[SUCESSO] Coluna 'periodo_dia' criada. Isso transforma a hora (numérica) em uma categoria (texto), o que é ótimo para encontrar regras.\")\n",
    "print(\"[ANÁLISE] Vamos ver a distribuição de transações por período:\")\n",
    "print(df['periodo_dia'].value_counts())\n",
    "\n",
    "# Criação da coluna 'valor_cat'\n",
    "print(\"\\n[AÇÃO] Criando a coluna 'valor_cat' para agrupar os valores das transações em faixas (Baixo, Medio, Alto)...\")\n",
    "df['valor_cat'] = pd.qcut(df['Valor'], q=3, labels=['Baixo', 'Medio', 'Alto'])\n",
    "print(\"[SUCESSO] Coluna 'valor_cat' criada. Agrupar valores ajuda o algoritmo a encontrar padrões como 'transações de baixo valor são suspeitas'.\")\n",
    "print(\"[ANÁLISE] Distribuição de transações por categoria de valor:\")\n",
    "print(df['valor_cat'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a652e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- FASE 3: Transformando para o Formato de 'Cesta de Compras' ---\n",
      "============================================================\n",
      "\n",
      "[OBJETIVO] O algoritmo que usaremos (Apriori) foi criado para analisar cestas de compras. Precisamos formatar nossos dados para que cada transação pareça uma 'cesta' e suas características (banco, status, etc.) pareçam 'itens' dentro dela.\n",
      "\n",
      "[SUCESSO] Dados convertidos para o formato de lista.\n",
      "[EXEMPLO] Uma transação que era uma linha na tabela, agora se parece com isto:\n",
      "['Pagador_Banco=BTG Pactual', 'Recebedor_Banco=Nubank', 'TipoChave=Telefone', 'Status=Concluída', 'periodo_dia=Noite', 'valor_cat=Alto', 'Anomalia=0']\n"
     ]
    }
   ],
   "source": [
    "# --- FASE 3: Transformando os Dados para o Formato de 'Cesta de Compras' ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- FASE 3: Transformando para o Formato de 'Cesta de Compras' ---\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[OBJETIVO] O algoritmo que usaremos (Apriori) foi criado para analisar cestas de compras. Precisamos formatar nossos dados para que cada transação pareça uma 'cesta' e suas características (banco, status, etc.) pareçam 'itens' dentro dela.\")\n",
    "\n",
    "df_transacional = df[['Pagador_Banco', 'Recebedor_Banco', 'TipoChave', 'Status', 'periodo_dia', 'valor_cat', 'Anomalia']]\n",
    "records = []\n",
    "for i in range(len(df_transacional)):\n",
    "    records.append([str(col) + \"=\" + str(val) for col, val in df_transacional.iloc[i].items()])\n",
    "\n",
    "print(f\"\\n[SUCESSO] Dados convertidos para o formato de lista.\")\n",
    "print(\"[EXEMPLO] Uma transação que era uma linha na tabela, agora se parece com isto:\")\n",
    "print(records[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a68209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- FASE 4: Encontrando Padrões Frequentes (Mineração) ---\n",
      "============================================================\n",
      "\n",
      "[OBJETIVO] Agora começa a 'mineração'. Vamos usar o algoritmo Apriori para ler todas as 'cestas' e encontrar combinações de 'itens' que aparecem com frequência.\n",
      "\n",
      "[SUCESSO] O algoritmo Apriori analisou os dados com um suporte mínimo de 0.2%.\n",
      "O que isso significa? Ele buscou combinações de características que aparecem em pelo menos 0.2% de todas as transações.\n",
      "\n",
      "[RESULTADO] Foram encontrados 10252 'itemsets' (combinações) frequentes.\n",
      "[AMOSTRA] Abaixo estão os 10 itemsets mais comuns encontrados:\n",
      "     support                        itemsets\n",
      "0   0.992382                    (Anomalia=0)\n",
      "28  0.336006              (Status=Estornada)\n",
      "27  0.334002              (Status=Concluída)\n",
      "67  0.333601  (Anomalia=0, Status=Estornada)\n",
      "39  0.333400                (valor_cat=Alto)\n",
      "41  0.333300               (valor_cat=Medio)\n",
      "40  0.333300               (valor_cat=Baixo)\n",
      "80  0.331596   (Anomalia=0, valor_cat=Medio)\n",
      "78  0.331295    (Anomalia=0, valor_cat=Alto)\n",
      "66  0.331295  (Anomalia=0, Status=Concluída)\n"
     ]
    }
   ],
   "source": [
    "# --- FASE 4: Encontrando Padrões Frequentes (Mineração) ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- FASE 4: Encontrando Padrões Frequentes (Mineração) ---\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[OBJETIVO] Agora começa a 'mineração'. Vamos usar o algoritmo Apriori para ler todas as 'cestas' e encontrar combinações de 'itens' que aparecem com frequência.\")\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df_onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "min_sup = 0.002\n",
    "frequent_itemsets = apriori(df_onehot, min_support=min_sup, use_colnames=True)\n",
    "print(f\"\\n[SUCESSO] O algoritmo Apriori analisou os dados com um suporte mínimo de {min_sup*100}%.\")\n",
    "print(\"O que isso significa? Ele buscou combinações de características que aparecem em pelo menos 0.2% de todas as transações.\")\n",
    "print(f\"\\n[RESULTADO] Foram encontrados {len(frequent_itemsets)} 'itemsets' (combinações) frequentes.\")\n",
    "print(\"[AMOSTRA] Abaixo estão os 10 itemsets mais comuns encontrados:\")\n",
    "print(frequent_itemsets.sort_values(by='support', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfae170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- FASE 5: Gerando as Regras de Associação ---\n",
      "============================================================\n",
      "\n",
      "[OBJETIVO] Com os 'itemsets' frequentes em mãos, podemos agora gerar as 'regras de associação', que são as pérolas da nossa análise. Elas têm o formato 'SE acontecer X, ENTÃO provavelmente acontecerá Y'.\n",
      "\n",
      "[SUCESSO] Regras geradas e filtradas para mostrar apenas aquelas que levam a uma anomalia.\n",
      "[RESULTADO] Encontramos 1 perfis de transações que são fortes indicadores de anomalia.\n",
      "\n",
      "[INTERPRETAÇÃO] Como ler a tabela de regras abaixo:\n",
      " - 'antecedents' (SE...): As condições da transação.\n",
      " - 'consequents' (ENTÃO...): O resultado, que no nosso caso é sempre 'Anomalia=1'.\n",
      " - 'confidence': A 'taxa de acerto' da regra. Um valor de 1.0 significa 100% de certeza.\n",
      " - 'lift': A 'força' da regra. Um lift de 12 significa que a regra é 12x mais provável de indicar uma anomalia do que o acaso. É a métrica mais importante!\n",
      "\n",
      "[AMOSTRA] As 5 regras mais fortes encontradas:\n",
      "                              antecedents consequents  antecedent support  \\\n",
      "79  (Recebedor_Banco=Banco Fantasma S.A.)  Anomalia=1            0.002005   \n",
      "\n",
      "    consequent support   support  confidence        lift  representativity  \\\n",
      "79            0.007618  0.002005         1.0  131.263158               1.0   \n",
      "\n",
      "    leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
      "79   0.00199         inf       0.994375  0.263158        1.0    0.631579  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- FASE 5: Gerando as Regras de Associação ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- FASE 5: Gerando as Regras de Associação ---\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[OBJETIVO] Com os 'itemsets' frequentes em mãos, podemos agora gerar as 'regras de associação', que são as pérolas da nossa análise. Elas têm o formato 'SE acontecer X, ENTÃO provavelmente acontecerá Y'.\")\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05)\n",
    "rules['consequents'] = rules['consequents'].apply(lambda x: list(x)[0]).astype(\"unicode\")\n",
    "rules_anomalia = rules[rules['consequents'] == 'Anomalia=1']\n",
    "rules_anomalia_sorted = rules_anomalia.sort_values(by='lift', ascending=False)\n",
    "\n",
    "print(\"\\n[SUCESSO] Regras geradas e filtradas para mostrar apenas aquelas que levam a uma anomalia.\")\n",
    "print(f\"[RESULTADO] Encontramos {len(rules_anomalia_sorted)} perfis de transações que são fortes indicadores de anomalia.\")\n",
    "print(\"\\n[INTERPRETAÇÃO] Como ler a tabela de regras abaixo:\")\n",
    "print(\" - 'antecedents' (SE...): As condições da transação.\")\n",
    "print(\" - 'consequents' (ENTÃO...): O resultado, que no nosso caso é sempre 'Anomalia=1'.\")\n",
    "print(\" - 'confidence': A 'taxa de acerto' da regra. Um valor de 1.0 significa 100% de certeza.\")\n",
    "print(\" - 'lift': A 'força' da regra. Um lift de 12 significa que a regra é 12x mais provável de indicar uma anomalia do que o acaso. É a métrica mais importante!\")\n",
    "print(\"\\n[AMOSTRA] As 5 regras mais fortes encontradas:\")\n",
    "print(rules_anomalia_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfeece2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- FASE 6: Caça às Anomalias e Geração do CSV Final ---\n",
      "============================================================\n",
      "\n",
      "[OBJETIVO] Agora, a parte mais prática! Vamos usar as regras que aprendemos para 'caçar' as transações suspeitas no nosso arquivo original e criar um relatório final em CSV.\n",
      "\n",
      "[PROCESSO] O código irá varrer o arquivo de transações e, para cada uma, verificar se ela corresponde a algum dos perfis de anomalia encontrados...\n",
      "\n",
      "[ANÁLISE CONCLUÍDA] 20 transações foram sinalizadas como potencialmente anômalas.\n",
      "\n",
      "============================================================\n",
      "--- RESULTADO FINAL ---\n",
      "============================================================\n",
      "\n",
      "[SUCESSO TOTAL!] Um novo arquivo chamado 'transacoes_sinalizadas_como_anomalias.csv' foi criado na mesma pasta deste script.\n",
      "Este arquivo contém APENAS as transações suspeitas e uma nova coluna 'motivo_sinalizacao' que explica por que cada uma foi marcada.\n",
      "\n",
      "[AMOSTRA DO ARQUIVO FINAL] Veja como ficou a saída:\n",
      "                DataHora    Valor    Pagador_Banco     Status  Anomalia  \\\n",
      "701  2025-07-09 11:41:56  4288.36      Banco Inter   Pendente         1   \n",
      "1101 2025-08-19 10:36:56  4874.61  Banco do Brasil  Concluída         1   \n",
      "1378 2025-07-22 11:06:56  2373.87  Banco do Brasil  Estornada         1   \n",
      "1395 2025-06-22 09:03:56  4107.17         Bradesco  Concluída         1   \n",
      "2431 2025-08-06 14:53:56  4540.49      Banco Safra   Pendente         1   \n",
      "2712 2025-07-09 19:19:56  1837.84           Nubank  Concluída         1   \n",
      "3210 2025-07-11 07:00:56   684.25      Banco Inter   Pendente         1   \n",
      "3327 2025-06-30 07:25:56  3540.43      BTG Pactual  Estornada         1   \n",
      "4174 2025-08-02 17:04:56   183.09    Itaú Unibanco   Pendente         1   \n",
      "4422 2025-07-30 20:34:56  3197.03    Itaú Unibanco  Concluída         1   \n",
      "\n",
      "                                     motivo_sinalizacao  \n",
      "701   Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "1101  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "1378  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "1395  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "2431  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "2712  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "3210  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "3327  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "4174  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n",
      "4422  Regra (Força: 131.26): Recebedor_Banco=Banco F...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- FASE 6: Caça às Anomalias e Geração do CSV Final ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- FASE 6: Caça às Anomalias e Geração do CSV Final ---\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[OBJETIVO] Agora, a parte mais prática! Vamos usar as regras que aprendemos para 'caçar' as transações suspeitas no nosso arquivo original e criar um relatório final em CSV.\")\n",
    "print(\"\\n[PROCESSO] O código irá varrer o arquivo de transações e, para cada uma, verificar se ela corresponde a algum dos perfis de anomalia encontrados...\")\n",
    "\n",
    "df['motivo_sinalizacao'] = None\n",
    "\n",
    "for index, rule in rules_anomalia_sorted.iterrows():\n",
    "    conditions = list(rule['antecedents'])\n",
    "    filtro_pandas = None\n",
    "    for i, condition in enumerate(conditions):\n",
    "        coluna, valor = condition.split('=', 1)\n",
    "        if df[coluna].dtype == 'int64': valor = int(valor)\n",
    "        elif df[coluna].dtype == 'float64': valor = float(valor)\n",
    "        if i == 0:\n",
    "            filtro_pandas = (df[coluna] == valor)\n",
    "        else:\n",
    "            filtro_pandas = filtro_pandas & (df[coluna] == valor)\n",
    "    motivo = f\"Regra (Força: {rule['lift']:.2f}): {' E '.join(conditions)}\"\n",
    "    df.loc[filtro_pandas & df['motivo_sinalizacao'].isnull(), 'motivo_sinalizacao'] = motivo\n",
    "\n",
    "df_anomalias_sugeridas = df.dropna(subset=['motivo_sinalizacao'])\n",
    "print(f\"\\n[ANÁLISE CONCLUÍDA] {len(df_anomalias_sugeridas)} transações foram sinalizadas como potencialmente anômalas.\")\n",
    "\n",
    "output_filename = 'transacoes_sinalizadas_como_anomalias.csv'\n",
    "df_anomalias_sugeridas.to_csv(output_filename, sep=';', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- RESULTADO FINAL ---\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n[SUCESSO TOTAL!] Um novo arquivo chamado '{output_filename}' foi criado na mesma pasta deste script.\")\n",
    "print(\"Este arquivo contém APENAS as transações suspeitas e uma nova coluna 'motivo_sinalizacao' que explica por que cada uma foi marcada.\")\n",
    "print(\"\\n[AMOSTRA DO ARQUIVO FINAL] Veja como ficou a saída:\")\n",
    "print(df_anomalias_sugeridas[['DataHora', 'Valor', 'Pagador_Banco', 'Status', 'Anomalia', 'motivo_sinalizacao']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
